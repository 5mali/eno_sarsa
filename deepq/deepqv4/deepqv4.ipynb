{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Q Network for ENO\n",
    "#Increase no. of actions to 10\n",
    "#Decrease Hidden Layer size to 30\n",
    "#Increase training for each year by 10 times\n",
    "#Changed Batch Size to 24\n",
    "#Increase Memory Capacity to one month of memory\n",
    "#update every two weeks\n",
    "#added choose_greedy_action() method\n",
    "#increase no. of iterations to 20\n",
    "\n",
    "#LIMIT ENO environment to have a HMAX = 1000\n",
    "#Change BOPT to 50% of BMAX\n",
    "#RESET Battery during training and testing\n",
    "\n",
    "#Broadcast rewards back in time\n",
    "\n",
    "#Decrease Battery Size to 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(230228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 24\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9              # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*2    # target update frequency (every two weeks)\n",
    "MEMORY_CAPACITY = 24*7*4      # store upto one month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,...,9)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    def __init__(self, year=2010):\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.TIME_STEPS = None\n",
    "        self.NO_OF_DAYS = None\n",
    "        \n",
    "        self.BMIN = 0.0\n",
    "        self.BMAX = 20000.0 #Battery capacity\n",
    "        self.BOPT = 0.5 * self.BMAX #Assuming 40% of battery is the optimal level\n",
    "        self.HMAX = 1000\n",
    "        \n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #matrix with forecast values for each day\n",
    "        \n",
    "        self.batt = None #battery variable\n",
    "        self.enp = None #enp at end of hr\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "    \n",
    "    #function to map total day energy into day_state\n",
    "    def get_day_state(self,tot_day_energy):\n",
    "        if (tot_day_energy < 2500):\n",
    "            day_state = 0\n",
    "        elif (2500 <= tot_day_energy < 5000):\n",
    "            day_state = 1\n",
    "        elif (5000 <= tot_day_energy < 8000):\n",
    "            day_state = 2\n",
    "        elif (8000 <= tot_day_energy < 10000):\n",
    "            day_state = 3\n",
    "        elif (10000 <= tot_day_energy < 12000):\n",
    "            day_state = 4\n",
    "        else:\n",
    "            day_state = 5\n",
    "        return int(day_state)\n",
    "\n",
    "    #function to get the solar data for the given year and prep it\n",
    "    def get_data(self):\n",
    "        filename = str(self.year)+'.csv'\n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(filename, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "        \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "        solar_energy = np.array([i *0.0165*1000000*0.15*1000/(60*60) for i in solar_radiation])\n",
    "        \n",
    "        #reshape solar_energy into no_of_daysx24 array\n",
    "        _senergy = solar_energy.reshape(-1,24)\n",
    "        _senergy[np.isnan(_senergy)] = 0 #convert missing data in CSV files to zero\n",
    "        self.senergy = np.clip(_senergy,0,self.HMAX) #limit the amount of harvested energy to HMAX\n",
    "        \n",
    "        \n",
    "        #create a perfect forecaster\n",
    "        tot_day_energy = np.sum(self.senergy, axis=1) #contains total energy harvested on each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_energy)\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        print(\"Environment is RESET\")\n",
    "        \n",
    "        self.day = 0\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.batt = self.BOPT #battery returns to optimal level\n",
    "        self.enp = self.BOPT - self.batt #enp is reset to zero\n",
    "        self.henergy = self.senergy[self.day][self.hr] \n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        state = [self.batt/self.BMAX, self.enp/(self.BMAX/2), self.henergy/self.HMAX, self.fcast/5] #normalizing all state values within [0,1] interval\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = \"RESET\"\n",
    "        return [state, reward, done, info]\n",
    "    \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        mu = 0\n",
    "        sig = 1000\n",
    "#         return ((1./(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((self.enp - mu)/sig, 2.)/2)) * 2000000)-400\n",
    "\n",
    "\n",
    "        if(np.abs(self.enp) <= 2400): #24hr * 100mW/hr\n",
    "            return ((1./(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((self.enp - mu)/sig, 2.)/2)) * 1000000)\n",
    "        else:\n",
    "            return -100 - 0.05*np.abs(self.enp)\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        info = \"OK\"\n",
    "#         print(\"Next STEP\")\n",
    "        \n",
    "        reward = 0\n",
    "        e_consumed = (action+1)*50\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.enp = self.BOPT - self.batt\n",
    "        \n",
    "        if(self.hr < self.TIME_STEPS - 1):\n",
    "            self.hr += 1\n",
    "            self.henergy = self.senergy[self.day][self.hr] \n",
    "        else:\n",
    "            if(self.day < self.NO_OF_DAYS -1):\n",
    "                reward = self.rewardfn() #give reward only at the end of the day\n",
    "                self.hr = 0\n",
    "                self.day += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                reward = self.rewardfn()\n",
    "                done = True\n",
    "                info = \"End of the year\"\n",
    "                \n",
    "        _state = [self.batt/self.BMAX, self.enp/(self.BMAX/2), self.henergy/self.HMAX, self.fcast/5]\n",
    "        return [_state, reward, done, info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 30)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(30, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_random_action(self):\n",
    "        action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        \n",
    "#         transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        np.insert(self.memory, index+1, transition_rec,0)\n",
    "        \n",
    "#         self.memory[index, :] = transition\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "eno = ENO(2010)\n",
    "# _LR =        [0.05, 0.03, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "_EPSILON =   [0.1,  0.2,  0.4,  0.7,   0.8,   0.8,   0.9,   0.9,   0.9,   0.9]   \n",
    "NO_OF_ITERATIONS = 10\n",
    "avg_reward_rec = np.empty(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    EPSILON = _EPSILON[iteration]\n",
    "#     LR = _LR[iteration]\n",
    "    print('\\nCollecting experience... Iteration', iteration+1)\n",
    "    print(\"EPSILON is\" , EPSILON )\n",
    "    print(\"LR is\" , LR)\n",
    "\n",
    "    s, r, done, info = eno.reset()\n",
    "    record = np.empty(4)\n",
    "    \n",
    "    transition_rec = np.zeros((eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "    \n",
    "    while True:\n",
    "    #     print([eno.day, eno.hr])\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "        # take action\n",
    "        s_, r, done, info = eno.step(a)\n",
    "    #     print([s_,r])\n",
    "    #     print(\"\\n\")\n",
    "      \n",
    "        temp_transitions = np.hstack((s, [a, r], s_))\n",
    "#         print(temp_transitions)\n",
    "        transition_rec[eno.hr-1,:] = temp_transitions\n",
    "        \n",
    "        transition_rec[:,5] = r #broadcast reward to all states\n",
    "        decay_factor = [i for i in (LAMBDA**n for n in reversed(range(1, eno.TIME_STEPS+1)))]\n",
    "        transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "        \n",
    "        \n",
    "        if eno.hr == 0:\n",
    "            eno.batt = eno.BOPT #resetting the battery to the optimal value for each day\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "#             print(transition_rec)\n",
    "#             print(\"\\n\")\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if done:\n",
    "            print(\"End of Data\")\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2]\n",
    "    reward_rec = reward_rec[reward_rec != 0]\n",
    "    print(\"Average reward =\", np.mean(reward_rec) )\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    \n",
    "    action_rec = record[:,3]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.plot(reward_rec,'y')\n",
    "    plt.ylabel(\"REWARD\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    ax1.set_ylim([-400,400])\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    plt.hist(action_rec, rwidth=0.75)#     plt.ylabel(\"Action\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "avg_reward_rec = np.delete(avg_reward_rec, 0, 0) #remove the first row which is garbage\n",
    "plt.plot(avg_reward_rec,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTesting...')\n",
    "s, r, done, info = eno.reset()\n",
    "test_record = np.empty(4)\n",
    "\n",
    "while True:\n",
    "#     print([eno.day, eno.hr])\n",
    "\n",
    "    a = dqn.choose_greedy_action(s)\n",
    "    \n",
    "    #state = [batt, enp, henergy, fcast]\n",
    "    test_record = np.vstack((test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "#     print(\"Action is\" , a)\n",
    "    # take action\n",
    "    s_, r, done, info = eno.step(a)\n",
    "#     print([s_,r])\n",
    "#     print(\"\\n\")\n",
    "    if eno.hr == 0:\n",
    "        eno.batt = eno.BOPT #resetting the battery to the optimal value for each day\n",
    "   \n",
    "    if done:\n",
    "        print(\"End of Data\")\n",
    "        break\n",
    "       \n",
    "    s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reward_rec = test_record[:,2]\n",
    "test_reward_rec = test_reward_rec[test_reward_rec != 0]\n",
    "plt.plot(test_reward_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_record[:,0],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Battery Percentage\n",
    "np.mean(test_record[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DAY in range(eno.NO_OF_DAYS):\n",
    "    START = DAY*24\n",
    "    END = START+24\n",
    "\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    st = fig.suptitle(\"DAY %s\" %(DAY))\n",
    "\n",
    "    ax1 = fig.add_subplot(141)\n",
    "    ax1.plot(test_record[START:END,0])\n",
    "    ax1.set_title(\"Battery\")\n",
    "    ax1.set_ylim([0,1])\n",
    "\n",
    "    ax2 = fig.add_subplot(142)\n",
    "    ax2.plot(test_record[START:END,1])\n",
    "    ax2.set_title(\"Harvested Energy\")\n",
    "    ax2.set_ylim([0,1])\n",
    "\n",
    "    ax3 = fig.add_subplot(144)\n",
    "    ax3.axis('off')\n",
    "    if END < (eno.NO_OF_DAYS*eno.TIME_STEPS):\n",
    "        plt.text(0.5, 0.5, \"REWARD = %.2f\\n\" %(test_record[END+1,2]),fontsize=14, ha='center')\n",
    "\n",
    "    ax4 = fig.add_subplot(143)\n",
    "    ax4.plot(test_record[START:END,3])\n",
    "    ax4.set_title(\"Action\")\n",
    "    ax4.set_ylim([0,N_ACTIONS])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    st.set_y(0.95)\n",
    "    fig.subplots_adjust(top=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
