{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_state = 0 # type of day\n",
    "\n",
    "np.random.seed(230228)\n",
    "HMAX = 3000 #Maximum harvested energy\n",
    "\n",
    "DMIN = 1 #20% duty cycle = 100 mWhr\n",
    "DMAX = 5 #100% duty cycle = 500 mWhr\n",
    "DSCALE = 100 #scale to convert action value to actual power consumption\n",
    "NMAX = DMAX * DSCALE #max energy consumption\n",
    "\n",
    "BMIN = 0.0\n",
    "BMAX = 40000.0\n",
    "BOPT = 0.6 * BMAX\n",
    "BINIT = BOPT\n",
    "\n",
    "no_of_batt_state = 0;\n",
    "no_of_dist_state = 42;\n",
    "no_of_henergy_state = 30;\n",
    "no_of_day_state = 6;\n",
    "no_of_actions = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2000\n",
    "\n",
    "N_ACTIONS = no_of_actions\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        print(\"Neural net\")\n",
    "        print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2010\n",
    "filename = str(year)+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skiprows=4 to remove unnecessary title texts\n",
    "#usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "solar_radiation = pd.read_csv(filename, skiprows=4, encoding='shift_jisx0213', usecols=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to numpy array\n",
    "solar_radiation = solar_radiation.values\n",
    "solar_energy = np.array([i *0.0165*1000000*0.15*1000/(60*60) for i in solar_radiation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape solar_energy into no_of_daysx24 array\n",
    "senergy = solar_energy.reshape(-1,24)\n",
    "senergy[np.isnan(senergy)] = 0 #convert missing data in CSV files to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfn(next_dist):\n",
    "    mu = 0\n",
    "    sig = 900\n",
    "    \n",
    "    if(np.abs(next_dist) <= 2400):\n",
    "        return ((1./(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((next_dist - mu)/sig, 2.)/2)) * 1000000)\n",
    "    else:\n",
    "        return -100 - 0.05*np.abs(next_dist)\n",
    "\n",
    "y = [rewardfn(x) for x in np.arange(-5000,5000)]\n",
    "plt.plot(np.arange(-5000,5000),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_henergy_state(henergy):\n",
    "    henergy_state = int(np.floor(henergy/100))\n",
    "    henergy_state = np.clip(henergy_state, 0, 30)\n",
    "       \n",
    "    return int(henergy_state)\n",
    "\n",
    "y = [get_henergy_state(x) for x in np.arange(0.0,3000.0,0.5)]\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_state(dist):\n",
    "    temp = np.ceil(np.abs(dist/1000))\n",
    "    \n",
    "    if (dist >= 0):\n",
    "        dist_state = 21 - temp\n",
    "    else:\n",
    "        dist_state = 20 + temp\n",
    "    \n",
    "    dist_state = np.clip(dist_state, 0, 41)\n",
    "    \n",
    "    return int(dist_state)\n",
    "    \n",
    "y = [get_dist_state(x) for x in np.arange(-40000,40000)]\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(dist_state, henergy_state,  day_state, q):\n",
    "    # This is how to choose an action\n",
    "    q_values = q[dist_state, henergy_state,  day_state, :]\n",
    "    max_q_value = np.max(q_values)\n",
    "    max_action = np.argmax(q_values)\n",
    "    if (np.random.uniform() < epsilon) or ((q_values == 0).all()):  # select randomly if exploring or \n",
    "                                                                         #if state-action values are ALL zero\n",
    "        action = np.random.randint(1,no_of_actions)\n",
    "    else:   # act greedy\n",
    "        action = max_action\n",
    "    return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to map total day energy into day_state\n",
    "def get_day_state(tot_day_energy):\n",
    "    if (tot_day_energy < 2500):\n",
    "        day_state = 0\n",
    "    elif (2500 <= tot_day_energy < 5000):\n",
    "        day_state = 1\n",
    "    elif (5000 <= tot_day_energy < 8000):\n",
    "        day_state = 2\n",
    "    elif (8000 <= tot_day_energy < 10000):\n",
    "        day_state = 3\n",
    "    elif (10000 <= tot_day_energy < 12000):\n",
    "        day_state = 4\n",
    "    else:\n",
    "        day_state = 5\n",
    "        \n",
    "    return int(day_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a perfect forecaster\n",
    "tot_day_energy = np.sum(senergy, axis=1) #contains total energy harvested on each day\n",
    "get_day_state = np.vectorize(get_day_state)\n",
    "forecast = get_day_state(tot_day_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "sorted_days = []\n",
    "for fcast in range(0,6):\n",
    "    fcast_days = ([i for i,x in enumerate(forecast) if x == fcast])\n",
    "    shuffle(fcast_days)\n",
    "    sorted_days.append(fcast_days)\n",
    "\n",
    "# print(sorted_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##print tot_day energy\n",
    "# f, (ax1, ax2) = plt.subplots(1,2,figsize=(20,5))\n",
    "# ax1.plot(tot_day_energy)\n",
    "# ax1.set_title('Total Energy')\n",
    "# ax2.plot(forecast)\n",
    "# ax2.set_title('Day State')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAX = 5 #maximum number of iterations\n",
    "\n",
    "#q_init is used for optimistic initialization of the Q-table to maximize exploration in the initial phases of training\n",
    "q_init = 200;\n",
    "q = np.zeros((no_of_dist_state, no_of_henergy_state, no_of_day_state, no_of_actions)) + q_init\n",
    "e = np.zeros((no_of_dist_state, no_of_henergy_state, no_of_day_state, no_of_actions))  \n",
    "\n",
    "for gamma in [0.8]: #discount factor\n",
    "    for alpha in [0.05]: #learning rate\n",
    "        for lmbda in [0.8]: #sarsa decay parameter\n",
    "            for epsilon in [0.1]: #e-greedy rate\n",
    "\n",
    "                for day in sorted_days[day_state]:\n",
    "                    iteration = 0\n",
    "                    day_enp_rec = np.zeros(0) #to record the ENP across the iteration\n",
    "                    reward_rec = np.zeros(0) #to record the reward across the iteration\n",
    "                    \n",
    "                    #print(\"Training Day: {}\" .format(day))\n",
    "                    #for each day iterate IMAX times\n",
    "                    while (iteration < IMAX):\n",
    "                        #print(\"Iteration Number: {}\" .format(iteration))\n",
    "\n",
    "                        next_batt = BINIT #battery is initialized\n",
    "                        #print(\"Battery Level: {} \" .format(batt))\n",
    "\n",
    "                        e = e*0 # eligibility traces are reset\n",
    "\n",
    "                        next_dist = BINIT - next_batt  #distance from BOPT. Required to calculate dist_state.\n",
    "                        next_henergy = senergy[day,5]\n",
    "                        next_day_state = forecast[day]\n",
    "                        action = np.random.randint(1,no_of_actions) #choose random action in the beginning\n",
    "\n",
    "                        NO_OF_DAYS = senergy.shape[0]\n",
    "                        TIME_STEPS = senergy.shape[1]\n",
    "                        for hr in range(TIME_STEPS):\n",
    "\n",
    "                            #Battery State AGNOSTIC\n",
    "                            #Battery Energy\n",
    "                            batt = next_batt\n",
    "            #                 print(\"Battery = {}\" .format(batt))\n",
    "                            #batt_state = int(get_batt_state(batt))\n",
    "                            #print(\"Battery State = {}\" .format(batt_state))\n",
    "\n",
    "                            #Distance from Energy Neutrality\n",
    "                            dist = next_dist\n",
    "            #                 print(\"EN Distance = {}\" .format(dist))\n",
    "                            dist_state = int(get_dist_state(dist))\n",
    "            #                 print(\"EN Distance State = {}\" .format(dist_state))\n",
    "\n",
    "                            #Harvested Energy\n",
    "                            henergy = next_henergy\n",
    "            #                 print(\"Harvested Energy = {}\" .format(henergy))\n",
    "                            henergy_state = int(get_henergy_state(henergy))\n",
    "            #                 print(\"Harvested Energy State = {}\" .format(henergy_state))\n",
    "\n",
    "                            #Weather Forecast State\n",
    "                            day_state =  next_day_state\n",
    "            #                 print(\"Weather Forecast State for day {} is {}.\".format(day, day_state))\n",
    "\n",
    "                            #STATE HAS BEEN DETERMINED\n",
    "                            state = np.array([dist_state, henergy_state,  day_state])\n",
    "\n",
    "                            #PRESENT ACTION\n",
    "            #                 print(\"Action is {}.\".format(action+1))\n",
    "\n",
    "                            #CHOOSE NEXT ACTION\n",
    "                            next_action = choose_action(dist_state, henergy_state, day_state, q)\n",
    "            #                 print(\"Next Action is {}.\".format(next_action+1))\n",
    "\n",
    "\n",
    "                            #EXECUTE ACTION\n",
    "                            next_batt = batt - action*100 + henergy\n",
    "                            next_dist = BINIT - next_batt\n",
    "                            next_dist_state = get_dist_state(next_dist)\n",
    "\n",
    "                            if (hr < TIME_STEPS-1): #if the hour is not the last hour of the day\n",
    "                                next_henergy = senergy[day, hr+1]\n",
    "                                next_henergy_state = get_henergy_state(next_henergy)\n",
    "                            else: #if the hour is the last hour of the day, then fetch energy from the first hour of the next day\n",
    "                                if(day < NO_OF_DAYS-1): #if the day is not the last day of the year\n",
    "                                    next_henergy = senergy[day+1, 0]\n",
    "                                    next_henergy_state = get_henergy_state(next_henergy)\n",
    "                                    next_day_state = forecast[day+1]\n",
    "                                else: #if the hour is the last hour of the last day of the year, then shoganai - just assume the next day will be the same as the present day.\n",
    "                                    next_henergy = senergy[day, 0]\n",
    "                                    next_henergy_state = get_henergy_state(next_henergy)\n",
    "                                    next_day_state = forecast[day]\n",
    "\n",
    "                            #RECEIVE REWARD\n",
    "                            if (hr == TIME_STEPS-1):\n",
    "                                reward = rewardfn(next_dist)\n",
    "            #                     print(\"Reward is {}.\".format(reward))\n",
    "            #                     print(\"ENP is {}.\".format(next_dist))\n",
    "                                day_enp_rec = np.append(day_enp_rec, next_dist) #record the ENP at the end of day\n",
    "                                reward_rec = np.append(reward_rec, reward) #record the ENP at the end of day\n",
    "\n",
    "\n",
    "                            #SARSA LAMBDA UPDATE\n",
    "                            if (hr == TIME_STEPS-1):\n",
    "                                del_ = reward - q[dist_state, henergy_state, day_state, action]\n",
    "            #                     print(\"del@end = {}\" .format(del_))\n",
    "                            else:\n",
    "                                del_ = gamma * q[next_dist_state, next_henergy_state, next_day_state, next_action] - q[dist_state, henergy_state, day_state, action]\n",
    "            #                     print(\"del = {}\" .format(del_))\n",
    "\n",
    "\n",
    "                            #ELIGIBILITY TRACE AND Q-TABLE UPDATE\n",
    "                            e[dist_state, henergy_state, day_state, action] += 1 #increase eligibility ONLY of the state-action pair just visited\n",
    "                            qerr = alpha*del_*e\n",
    "                            q = q + qerr\n",
    "                            e = gamma*lmbda*e #decay eligibility trace for ALL state-action pairs by a fixed rate\n",
    "\n",
    "                            #UPDATES FOR NEXT TIME STEPS\n",
    "                            action = next_action\n",
    "                            batt = next_batt\n",
    "                            dist = next_dist\n",
    "                            henergy = next_henergy\n",
    "\n",
    "            #                 print(\"END OF HOUR {}\\n\\n\".format(hr))\n",
    "                        #END OF HOUR FOR LOOP\n",
    "                        iteration += 1\n",
    "                    #END OF ITERATION FOR LOOP\n",
    "                    #\n",
    "                    #\n",
    "                    #\n",
    "                    #\n",
    "                    #\n",
    "                    #AFTER COMPLETING ALL THE ITERATIOINS FOR TRAINING, WE TEST BY IMPLEMENTING THE GREEDY Q-TABLE\n",
    "                    \n",
    "                    batt_rec = np.zeros(0) #to record the battery level over a day after all iterations have finished\n",
    "                    action_rec = np.zeros(0) #to record the duty cycle over a day after all iterations have finished\n",
    "                    dist_rec = np.zeros(0) #to record the ENP at every hour of the day after all iterations have finished\n",
    "                    \n",
    "                    next_batt = BINIT #battery is initialized\n",
    "                    next_dist = BINIT - next_batt  #distance from BOPT. Required to calculate dist_state.\n",
    "                    next_henergy = senergy[day,5]\n",
    "                    next_day_state = forecast[day]\n",
    "                    action = np.random.randint(1,no_of_actions) #choose random action in the beginning\n",
    "\n",
    "                    for hr in range(TIME_STEPS):\n",
    "                        \n",
    "                        action_rec = np.append(action_rec, action) #record the action\n",
    "                        \n",
    "                        #Battery State AGNOSTIC\n",
    "                        #Battery Energy\n",
    "                        batt = next_batt\n",
    "                        batt_rec = np.append(batt_rec, batt) #record the battery values\n",
    "        #                 print(\"Battery = {}\" .format(batt))\n",
    "                        #batt_state = int(get_batt_state(batt))\n",
    "                        #print(\"Battery State = {}\" .format(batt_state))\n",
    "\n",
    "                        #Distance from Energy Neutrality\n",
    "                        dist = next_dist\n",
    "        #                 print(\"EN Distance = {}\" .format(dist))\n",
    "                        dist_state = int(get_dist_state(dist))\n",
    "        #                 print(\"EN Distance State = {}\" .format(dist_state))\n",
    "                        dist_rec = np.append(dist_rec, dist) #record the ENP values\n",
    "\n",
    "                        #Harvested Energy\n",
    "                        henergy = next_henergy\n",
    "        #                 print(\"Harvested Energy = {}\" .format(henergy))\n",
    "                        henergy_state = int(get_henergy_state(henergy))\n",
    "        #                 print(\"Harvested Energy State = {}\" .format(henergy_state))\n",
    "\n",
    "                        #Weather Forecast State\n",
    "                        day_state =  next_day_state\n",
    "        #                 print(\"Weather Forecast State for day {} is {}.\".format(day, day_state))\n",
    "\n",
    "                        #STATE HAS BEEN DETERMINED\n",
    "#                         state = np.array([dist_state, henergy_state,  day_state])\n",
    "\n",
    "                        #PRESENT ACTION\n",
    "        #                 print(\"Action is {}.\".format(action+1))\n",
    "\n",
    "                        #CHOOSE NEXT ACTION\n",
    "                        q_values = q[dist_state, henergy_state,  day_state, :]\n",
    "                        max_q_value = np.max(q_values)\n",
    "                        next_action = np.argmax(q_values) #choose next action greedily\n",
    "        #                 print(\"Next Action is {}.\".format(next_action+1))\n",
    "\n",
    "\n",
    "                        #EXECUTE ACTION\n",
    "                        next_batt = batt - action*100 + henergy\n",
    "                        next_dist = BINIT - next_batt\n",
    "                        next_dist_state = get_dist_state(next_dist)\n",
    "\n",
    "                        if (hr < TIME_STEPS-1): #if the hour is not the last hour of the day\n",
    "                            next_henergy = senergy[day, hr+1]\n",
    "                            next_henergy_state = get_henergy_state(next_henergy)\n",
    "                        else: #if the hour is the last hour of the day, then fetch energy from the first hour of the next day\n",
    "                            if(day < NO_OF_DAYS-1): #if the day is not the last day of the year\n",
    "                                next_henergy = senergy[day+1, 0]\n",
    "                                next_henergy_state = get_henergy_state(next_henergy)\n",
    "                                next_day_state = forecast[day+1]\n",
    "                            else: #if the hour is the last hour of the last day of the year, then shoganai - just assume the next day will be the same as the present day.\n",
    "                                next_henergy = senergy[day, 0]\n",
    "                                next_henergy_state = get_henergy_state(next_henergy)\n",
    "                                next_day_state = forecast[day]\n",
    "\n",
    "                                                             \n",
    "                        #UPDATES FOR NEXT TIME STEPS\n",
    "                        action = next_action\n",
    "                        batt = next_batt\n",
    "                        dist = next_dist\n",
    "                        henergy = next_henergy\n",
    "                    #END OF GREEDY IMPLEMENTATION FOR THE DAY\n",
    "                    \n",
    "                    treward = rewardfn(dist) #reward at the end of the day\n",
    "                    tenp = dist #TENP at the end of the day\n",
    "\n",
    "                    #PRINT RESULTS OF TRAINING FOR THE DAY\n",
    "                    fig = plt.figure(figsize=(15,5))\n",
    "                    st = fig.suptitle(\"DAY = %s \\n alpha = %s  gamma = %s  lambda = %s epsilon = %s \" %(day, alpha, gamma, lmbda, epsilon))\n",
    "                    TIME_AXIS = np.arange(0,TIME_STEPS)\n",
    "\n",
    "\n",
    "                    ax1 = fig.add_subplot(1,4,1)\n",
    "                    ax1.plot(reward_rec,'y')\n",
    "                    plt.ylabel(\"REWARD\")\n",
    "                    plt.xlabel(\"Iteration #\")\n",
    "                    ax1.set_ylim([-1000,500])\n",
    "\n",
    "                    ax2a = fig.add_subplot(1,4,2)\n",
    "                    ax2a.plot(TIME_AXIS, senergy[day,:],'g')\n",
    "                    plt.ylabel(\"Harvested Energy\")\n",
    "                    plt.xlabel(\"Hour\")\n",
    "                    ax2a.set_ylim([0,HMAX])\n",
    "\n",
    "                    ax2b = ax2a.twinx()\n",
    "                    ax2b.plot(TIME_AXIS, batt_rec,'r')\n",
    "                    ax2b.plot(TIME_AXIS, np.ones_like(batt_rec)*BINIT,'r--')\n",
    "                    plt.ylabel(\"Battery Level\")\n",
    "                    plt.xlabel(\"Hour\")\n",
    "                    MARGIN = 500*24\n",
    "                    ax2b.set_ylim([BOPT-MARGIN,BOPT+MARGIN])\n",
    "\n",
    "    #                     ax3 = fig.add_subplot(1,4,3)\n",
    "    #                     ax3.plot(dist_rec,'y')\n",
    "    #                     ax3.plot(np.zeros_like(dist_rec),'y--')\n",
    "    #                     plt.ylabel(\"ENP\")\n",
    "    #                     plt.xlabel(\"Hour\")\n",
    "    #                     ax3.set_ylim([-BMAX/2,BMAX/2])\n",
    "\n",
    "                    ax4 = fig.add_subplot(1,4,3)\n",
    "                    ax4.plot(action_rec+1,'b') #action 0 means dutcycle 1. It's more intuitive to display it like this in the plot.\n",
    "                    plt.ylabel(\"DUTYCYCLE\")\n",
    "                    plt.xlabel(\"Hour\")\n",
    "                    ax4.set_ylim([0,DMAX])\n",
    "\n",
    "#                     ax5 = fig.add_subplot(1,4,4)\n",
    "#                     plt.text(0.5, 0.5, \"REWARD = %.2f\\n\\n TENP = %.2f\" %(treward, tenp),fontsize=14, ha='center')\n",
    "\n",
    "                    \n",
    "                    greedy = np.empty_like(q[:,:,0,0])\n",
    "                    for i,j in np.ndindex(greedy.shape):\n",
    "                        greedy[i,j] = np.argmax(q[i,j,day_state,:])\n",
    "                    my_cmap = ListedColormap(['w', 'y', 'g','b','m'])\n",
    "\n",
    "                    ax6 = fig.add_subplot(1,4,4)\n",
    "                    cax = ax6.matshow(greedy, cmap=my_cmap)\n",
    "                    fig.colorbar(cax)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    fig.tight_layout()\n",
    "                    st.set_y(0.95)\n",
    "                    fig.subplots_adjust(top=0.75)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    \n",
    "                #END OF DAY LOOP\n",
    "            #end of epsilon loop\n",
    "        #end of lmbda loop\n",
    "    #end of alpha loop\n",
    "#end of gamma loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_AXIS = np.arange(0,TIME_STEPS)\n",
    "print(TIME_AXIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"qtable_daystate_v3_\" + str(day_state) +\".npy\"\n",
    "np.save(filename,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_qdata = q[:,:,day_state,:]\n",
    "qdata = np.where(_qdata==q_init,0,_qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(10,10))\n",
    "ax1 = fig2.add_subplot(321, projection = '3d')\n",
    "ax2 = fig2.add_subplot(322, projection = '3d')\n",
    "ax3 = fig2.add_subplot(323, projection = '3d')\n",
    "ax4 = fig2.add_subplot(324, projection = '3d')\n",
    "ax5 = fig2.add_subplot(325, projection = '3d')\n",
    "\n",
    "_x = np.arange(no_of_dist_state)\n",
    "_y = np.arange(no_of_henergy_state)\n",
    "_xx, _yy = np.meshgrid(_x, _y)\n",
    "x, y = _xx.ravel(), _yy.ravel()\n",
    "\n",
    "top = qdata[x,y,0]\n",
    "bottom = np.zeros_like(top)\n",
    "width = depth = 1\n",
    "\n",
    "ax1.bar3d(x, y, bottom, width, depth, qdata[x,y,0], shade=True, color = 'r')\n",
    "ax2.bar3d(x, y, bottom, width, depth, qdata[x,y,1], shade=True, color = 'y')\n",
    "ax3.bar3d(x, y, bottom, width, depth, qdata[x,y,2], shade=True, color = 'g')\n",
    "ax4.bar3d(x, y, bottom, width, depth, qdata[x,y,3], shade=True, color = 'b')\n",
    "ax5.bar3d(x, y, bottom, width, depth, qdata[x,y,4], shade=True, color = 'm')\n",
    "\n",
    "for ax in [ax1,ax2,ax3,ax4,ax5]:\n",
    "    ax.set_xlabel('Distance')\n",
    "    ax.set_ylabel('Harvested Energy')\n",
    "\n",
    "fig2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just checking if the file is there and stored properly\n",
    "q = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print only the greedy actions as a colormap\n",
    "greedy = np.empty_like(q[:,:,0,0])\n",
    "\n",
    "for i,j in np.ndindex(greedy.shape):\n",
    "    greedy[i,j] = np.argmax(q[i,j,day_state,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig3 = plt.figure(figsize=(15,15))\n",
    "\n",
    "my_cmap = ListedColormap(['k', 'y', 'g','b','m'])\n",
    "\n",
    "ax1 = fig3.add_subplot(111)\n",
    "cax = ax1.matshow(greedy, cmap=my_cmap)\n",
    "fig3.colorbar(cax)\n",
    "\n",
    "# _x = np.arange(no_of_dist_state)\n",
    "# _y = np.arange(no_of_henergy_state)\n",
    "# _xx, _yy = np.meshgrid(_x, _y)\n",
    "# x, y = _xx.ravel(), _yy.ravel()\n",
    "\n",
    "# bottom = np.zeros_like(top)\n",
    "# width = depth = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
